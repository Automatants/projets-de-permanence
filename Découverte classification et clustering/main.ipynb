{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovvEtcCS1rH6"
   },
   "source": [
    "# Projet de permanence : Découverte des outils de classification et de clustering\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Un peu de curiosité et de patience feront l'affaire ! ;)\n",
    "\n",
    "Plus sérieusement, il vous faut soit [Google colab'](https://colab.research.google.com/), soit un environnement Python avec `tensorflow`, `numpy`, `keras`et `scikit-learn`.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "L'objectif de ce sujet est de vous faire découvrir les librairies **Keras** et **scikit-learn** en vous permettant d'avoir vos premiers résultats de **Machine Learning** avec le dataset **CIFAR10**.\n",
    "\n",
    "Nous allons abordés les notions suivantes : les couches denses, les CNNs, les auto-encodeurs, le clustering, ...\n",
    "\n",
    "Le plan est le suivant :\n",
    "\n",
    "1. **La classification supervisée :** Prédiction du label d'une image grâce à un réseau de neurones\n",
    "2. **Les autoencodeurs :** Concept et 3 applications qui vont changer votre vie !\n",
    "3. **La classification non-supervisée :** Le secret derrière Argos.\n",
    "\n",
    "Ci-dessous une petite FAQ pour vous présenter certains points. Si vous avez des questions ou des remarques, n'hésitez pas à les poser aux 2As présents en perm' ou sur le groupe Automatants. Good luck, have fun !\n",
    "\n",
    "## FAQ\n",
    "\n",
    "### Comment ça marche un fichier Jupyter ?\n",
    "\n",
    "Les fichiers Jupyter vous permettent d'exécuter du Python depuis votre navigateur, tout en pouvant alterner morceaux de code et morceaux de textes. Afin d'éxécuter du code, il faut appuyer sur `Crtl` + `R` (Reste sur le bloc courant) ou `Shift` + `R` (Exécute le bloc courant et passe au bloc suivant).\n",
    "\n",
    "Tous les `print` ou `plot` que vous ferez s'afficheront directement en dessus du bloc de code.\n",
    "\n",
    "### Qu'est-ce qu'un dataset ?\n",
    "\n",
    "Un **dataset** est un jeu de données, qui généralement associe à des données d'entrée une ou plusieurs données de sortie.\n",
    "\n",
    "Par exemple, le dataset MNIST associe à des images en nuance de gris le chiffre qu'elle représente.\n",
    "\n",
    "Ici, nous allons manipulé le dataset CIFAR10 qui à des images en couleur associe le nom de de ce qu'elle représente (Plus de détails dans la suite).\n",
    "\n",
    "La construction d'un dataset bien labelisé et conséquent est une des difficultés majeurs en Machine Learning. Heureusement pour nous, de nombreux datasets existent et sont mis à disposition de tous !\n",
    "\n",
    "### Mais c'est quoi Keras?\n",
    "\n",
    "**Keras** est une libraire qui permet de rapidement créer des réseaux de neurones et de les entrainer. De nombreuses couches sont déjà codées, tel que les couches denses (du multi-perceptron), des CNNs, des couches récurrentes, etc., ce qui permet de n'avoir qu'à les assembler. Keras inclut également plusieurs datasets, ce qui est très pratique pour tester des algorithmes.\n",
    "\n",
    "### Et scikit-learn ?\n",
    "\n",
    "De son côté **scikit-learn** propose beaucoup d'outils pour faire de l'analyse prédictive de données (Classification, régression, clustering, réduction de dimension, etc. *Beaucoup de termes barbares que vous allez découvrir dans la suite*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DoAzN2f1rH9"
   },
   "source": [
    "# Import des librairies et des données\n",
    "\n",
    "## Les librairies\n",
    "\n",
    "Commençons par importer les librairies dont nous aurons besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1G1uGxc1rIA"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0raS2D1U1rIR"
   },
   "source": [
    "## Le dataset\n",
    "\n",
    "Importons maintenant le dataset. Nous allons utiliser CIFAR10, dont voilà une description :\n",
    "\n",
    "> L'ensemble de données CIFAR10 contient 60 000 images couleur de 32x32 dans 10 classes, avec 6 000 images dans chaque classe. L'ensemble de données est divisé en 50 000 images d'entraînement et 10 000 images de test. Les classes sont mutuellement exclusives et il n'y a pas de chevauchement entre elles.\n",
    "\n",
    "Pour rappel, il est important de séparer les données en données d'entrainement et données de test. Cela permet de constater si notre modèle a effectué un sur-apprentissage (overfitting) et est incapable de généraliser ce qu'il a appris.\n",
    "\n",
    "**Keras** inclut de nombreux datasets dans son modèle `datasets` ([liste des datasets](https://keras.io/api/datasets/)) que l'on peut importer avec la fonction `load_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFmrSsva1rIU",
    "outputId": "419a8389-43aa-4f82-99d5-b47cec31871e"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdaXvFLx1rIe"
   },
   "source": [
    "Les valeurs des pixels varient entre 0 et 255. Nous allons les normalisé pour qu'elles soient entre 0 et 1, ce qui est plus adapté pour les réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9RUtkkp1rIg"
   },
   "outputs": [],
   "source": [
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaNhvUQt1rIn"
   },
   "source": [
    "Visualisons un peu toutes ces images !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "5jSND_dO1rIp",
    "outputId": "a81d3779-381c-4c14-e1f2-07f5d9aaa94b"
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9ZsfFfD1rIx"
   },
   "source": [
    "Parfait, nous pouvons maintenant commencer les choses sérieuses !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v36aaMiL1rI0"
   },
   "source": [
    "# Classification supervisée\n",
    "\n",
    "## Qu'est-ce que la classification supervisée ?\n",
    "\n",
    "L'idée est de trouver une fonction de prédiction à partir des données annotées. Ici, cela revient à trouver la fonction qui à chaque image associe son label.\n",
    "\n",
    "Cela est différent de l'apprentissage non-supervisée que l'on fera dans la suite.\n",
    "\n",
    "## Comment allons-nous faire ?\n",
    "\n",
    "Les images font 32 pixels par 32 avec 3 valeurs pour chaque couleur (**R**ouge, **V**ert, **B**leu ou **RVB** en français et **RGB** en anglais). Nous avons donc 32 * 32 * 3 = 3072 paramètres ...\n",
    "\n",
    "Nous allons donc utiliser des réseaux de neurones pour approximer notre fonction de prédiction !\n",
    "\n",
    "## Avec un réseau dense\n",
    "\n",
    "### Rappel théorique\n",
    "\n",
    "Si vous assistez aux premières formations, vous savez ce qu'est un réseau dense et si vous avez assisté au TP Multiperceptron, vous avez même codé votre propre réseau dense !\n",
    "\n",
    "Pour ceux qui ne s'en souvienne plus, un réseau de neurone dense est composée de plusieurs couches, elles-mêmes composées de plusieurs noeuds. Chaque noeud est relié à tous les noeuds de la couche précédente, dont il pondère la somme des valeurs par des poids, et décale la valeur obtenue par un biais. Finalement, pour éviter que l'ensemble de toutes les couches se résument à une seule couche, on casse la linéarité en associant à chaque couche une fonction d'activation (sigmoïde, tanh, relu, etc.).\n",
    "\n",
    "C'est une explication très très succinte. Pour mieux comprendre ce que c'est, vous pouvez revoir la formation sur les premières formations, ou il y a ces excellentes vidéos de [Science4All, Les réseaux de neurones | Intelligence artificielle 41](https://www.youtube.com/watch?v=8qL2lSQd9L8) (FR) et de [3Blue1Brown, But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk) (ANG, mais très visuel).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/220px-Neural_network.svg.png)\n",
    "\n",
    "La première couche est appelée `Input layer`, car c'est là que sont insérées les données d'entrée. La dernière couche est appelée `Output layer` et correspond aux valeurs de sortie. Les couches intermédiaires sont appelées `Hidden layer`.\n",
    "\n",
    "### En pratique\n",
    "\n",
    "Pour créer un modèle avec **Keras**, on peut utiliser `keras.models.Sequential()`.\n",
    "\n",
    "**Instruction :** Créer un modèle que vous assignerez à `model_dense1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIzviHUn1rI1"
   },
   "outputs": [],
   "source": [
    "model_dense1 = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REuZJ2tF1rI-"
   },
   "source": [
    "Il est maintenant temps d'ajouter les premières couches à ce modèle. On pourra utiliser :\n",
    "- `model_dense1.add(layer)` qui ajoute layer à la suite des couches du modèle.\n",
    "\n",
    "\n",
    "- `keras.layers.Dense(n_noeuds, activation='...')` qui renvoie une couche dense avec `n_noeuds` noeuds et comme fonction d'activation celle que vous avez précisez (les options possibles sont notamment `sigmoid`, `tanh`, `relu`, `softmax`)\n",
    "\n",
    "  Plus d'infos dans la doc de Keras : [Dense Layer](https://keras.io/api/layers/core_layers/dense/).\n",
    "\n",
    "\n",
    "- `keras.layers.Flatten(input_shape=shape)` qui renvoie une couche qui *aplatit* les données d'entrée en une liste à la sortie.\n",
    "\n",
    "  `input_shape` est un tuple qui permet de préciser le format des données d'entrée (Exemple (32, 32, 3) pour les images de CIFAR10).\n",
    "\n",
    "Maintenant, réfléchissons un peu. Nos données d'entrée sont de la forme (32, 32, 3). Hors une couche dense ne peut prendre qu'une liste en entrée ... Il va donc falloir utiliser utiliser une première couche `Flatten` pour avoir le bon format.\n",
    "\n",
    "**Instruction :** Rajoutez une couche `Flatten` au modèle.\n",
    "\n",
    "*Note :* On pourrait également modifier les données d'entrée, mais vu que dans la suite, on va remanier les images, ça ne serait pas pratique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDbNW6xZ1rJA"
   },
   "outputs": [],
   "source": [
    "model_dense1.add(keras.layers.Flatten(input_shape=(32, 32, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-loxVDsI1rJF"
   },
   "source": [
    "À la sortie de cette, nous avons donc une liste de 32 * 32 * 3 valeurs. On peut donc enfin mettre une première couche cachée !\n",
    "\n",
    "**Instruction :** Rajoutez une couche `Dense` avec 512 noeuds et la fonction d'activation `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfYQExy11rJH"
   },
   "outputs": [],
   "source": [
    "model_dense1.add(keras.layers.Dense(512, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9IYVw0O1rJO"
   },
   "source": [
    "Ajoutons maintenant la couche de sortie, on a 10 classes possibles. Notre dernière couche aura donc 10 noeuds, et on aimerait que notre réseau prédise la probabilité de chaque classe, on utilisera donc la fonction d'activation `softmax`.\n",
    "\n",
    "**Instruction :** Rajoutez une couche `Dense` avec 10 noeuds et la fonction d'activation `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "953uQvae1rJP"
   },
   "outputs": [],
   "source": [
    "model_dense1.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qiu_J-Li1rJX"
   },
   "outputs": [],
   "source": [
    "model_dense1 = keras.models.Sequential()\n",
    "model_dense1.add(layers.Dense(1024, activation='relu'))\n",
    "model_dense1.add(layers.Dense(512, activation='relu'))\n",
    "model_dense1.add(layers.Dense(256, activation='relu'))\n",
    "model_dense1.add(layers.Dense(128, activation='relu'))\n",
    "model_dense1.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "mdk7Iqyd1rJe",
    "outputId": "56d7789f-7bc9-490d-a475-ea1739835c5f"
   },
   "outputs": [],
   "source": [
    "#model_dense1.summary()\n",
    "model_dense1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model_dense1.fit(train_images, train_labels, epochs=2,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model_dense1.evaluate(test_images, test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8aozBTX1rJk"
   },
   "source": [
    "### Avec des CNNs\n",
    "\n",
    "Bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLRLQ0GP1rJm"
   },
   "outputs": [],
   "source": [
    "model_cnn = models.Sequential()\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.Flatten())\n",
    "model_cnn.add(layers.Dense(64, activation='relu'))\n",
    "model_cnn.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgzykQKY1rJt"
   },
   "outputs": [],
   "source": [
    "model_cnn.summary()\n",
    "model_cnn.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model_cnn.fit(train_images, train_labels, epochs=5,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model_cnn.evaluate(test_images, test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llgMhj5x1rJ0"
   },
   "source": [
    "## Auto-encodeurs et applications diverses\n",
    "\n",
    "Je vais sûrement virer le réseau dense et garder que le CNN ici, vu que les CNNs auront été introduit ci-dessus.\n",
    "\n",
    "\n",
    "## Auto-encodeur\n",
    "\n",
    "Les auto-encodeurs sont une famille de réseaux de neurones particuliers qui ont pour premier objectif de réduire la dimension de l'espace des données qui nous intéressent. Il se décomposent en deux parties :\n",
    "\n",
    "*   Un encodeur qui doit apprendre la représantation des données d'entrée qui permet de passer de la dimension initiale à la dimension réduite.\n",
    "*   Un décodeur qui doit reproduire l'entrée le plus fidèlement possible à partir de la représentation que donne l'encodeur.\n",
    "\n",
    "On retrouve donc un structure caractéristique en \"goulot d'étranglement\" où les couches aux extrémités sont de la même dimension que la taille des données à caractériser et où la couche centrale est de la dimension de reduction souhaitée qui contiendra la représentation des entrées.\n",
    "\n",
    "![](https://www.researchgate.net/publication/318204554/figure/fig1/AS:512595149770752@1499223615487/Autoencoder-architecture.png)\n",
    "\n",
    "Pour entrainer ce réseau, on va seulement utilisée les données d'entrées : pas besoin donc de labéliser les données, on laisse la descente de gradient faire tout le travail : il s'agit d'un apprentissage non-supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y40pysmG1rJ2"
   },
   "source": [
    "### Avec un réseau dense\n",
    "\n",
    "Bla bla ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWa2EjZn1rJ3"
   },
   "outputs": [],
   "source": [
    "latent_dim = 512\n",
    "\n",
    "class AutoencoderDense(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1024, activation='relu'),\n",
    "            layers.Dense(latent_dim, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(1024, activation='relu'),\n",
    "            layers.Dense(32 * 32 * 3, activation='sigmoid'),\n",
    "            layers.Reshape((32, 32, 3))\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpFli5kv1rJ-"
   },
   "source": [
    "### Avec des CNNs\n",
    "\n",
    "Bla bla ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9VBK3d51rJ_"
   },
   "outputs": [],
   "source": [
    "class AutoencoderCNN(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            # layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "            # layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            layers.Flatten(),\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Reshape((8, 8, 16)),\n",
    "            # layers.UpSampling2D((2, 2)),\n",
    "            # layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2D(3, (3, 3), padding='same', activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtudBdul1rKE"
   },
   "source": [
    "Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8CudU931rKF"
   },
   "outputs": [],
   "source": [
    "train = True \n",
    "autoencoder_cnn = None\n",
    "if train:\n",
    "    autoencoder_cnn = AutoencoderCNN(latent_dim)\n",
    "    autoencoder_cnn.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "    autoencoder_cnn.fit(train_images, train_images,\n",
    "                    epochs=1,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_images, test_images))\n",
    "\n",
    "#    autoencoder_cnn.save('./models/model_cnn1')\n",
    "#else:\n",
    "#    autoencoder_cnn = models.load_model('./models/model_cnn1')\n",
    "#    results = autoencoder_cnn.evaluate(test_images, test_images, verbose=2)\n",
    "#    print('test loss : ', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Eg86Zu1rKK"
   },
   "source": [
    "Voir ce que l'auto-encodeur comprend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3V-UgOt1rKN"
   },
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder_cnn.encoder(test_images).numpy()\n",
    "decoded_imgs = autoencoder_cnn.decoder(encoded_imgs).numpy()\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[i])\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3TeVZOq1rKS"
   },
   "source": [
    "### Applications diverses d'un auto-encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w34JJz11rKS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgmX_Niv1rKY"
   },
   "source": [
    "# - Passer le truc en \"mode TP\" avec des instructions.\n",
    "# - Problème dans le niveau des titres je crois\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fozIN64p1rKZ"
   },
   "source": [
    "## Analyse des caractéristiques profondes de l'auto-encodeur et clustering\n",
    "\n",
    "**Dans cette partie vous serez amenés à manipuler des méthodes et concepts que vous n'avez peut-être jamais vus et que nous n'avons pas encore expliqués dans les formations. Ce TP va vous permettre de les utiliser mais il n'y aura que très peu d'explications théoriques. On vous invite à lire les références que l'on vous donne pour comprendre le fonctionnement général des algorithmes. La plupart de ces méthodes vous seront présentées d'un point de vue plus théorique plus tard.**\n",
    "\n",
    "###Vecteurs latents\n",
    "\n",
    "Les vecteurs que l'on obtient après compression par l'auto-encodeur sont appelés **vecteurs latents**, ou **caractéristiques profondes**, ou **deep features**.\n",
    "Ces termes ne sont pas réservés aux auto-encodeurs, il s'agit de n'importe quels vecteurs de dimension réduite provenant d'un réseau profond.\n",
    "\n",
    "Ces vecteurs sont très utiles car ils sont de petites dimensions mais sont porteurs d'information sémantique. Autrement dit, ils \"encodent\" les données d'entrée du réseau, et sont un condensé des informations importantes.\n",
    "\n",
    "> Prenons un exemple.\n",
    "> Vous entrainez un réseau à reconnaitre des voitures (donc supervisé). Une fois que votre réseau est efficace sur des voitures, vous le coupez quelques couches avant la fin. Vous obtenez un réseau qui prend en entrée des images et donne en sortie un vecteur de petite dimension, disons 64. Comme votre réseau aura appris à extraire les informations utiles pour reconnaitre une voiture (roues, forme, etc), ces vecteurs comporteront toutes les informations nécessaires à reconnaitre des voitures.\n",
    "Ensuite, si vous donnez à ce réseau des images de motos et de camions, une simple comparaison des vecteurs de sortie pourra indiquer si il s'agit d'une moto ou d'un camion. Il sera évidemment toujours peu efficace pour reconnaitre des objets éloignés des voitures, comme des visages par exemple. \n",
    "\n",
    "### But de cette dernière partie\n",
    "\n",
    "Le but général de cette dernière partie est de vous montrer quelques outils classiques de machine learning et d'analyse des données. Dans la continuité de la première partie, l'idée est de pouvoir analyser les vecteurs latents provenants de l'auto-encoder. En particulier, vous serez capables d'appliquer un clustering non supervisé sur l'espace latent pour pouvoir retrouver les 10 classes de CIFAR-10, sans jamais utiliser les labels.\n",
    "\n",
    "Pour cela on va voir chacun de ces outils de machine learning en s'entrainant sur la dataset \"Wine\" qui contient 3 classes de différents vins, dont chacun a 13 attributs (principalement les taux de présence de composés chimiques).\n",
    "\n",
    "Nous verrons :\n",
    "- l'importance de la normalisation des données\n",
    "- la visualisation par t-SNE\n",
    "- la réduction de dimensions par ACP\n",
    "- le clustering par K-means\n",
    "- le clustering par agglomération\n",
    "\n",
    "Ensuite, vous pourrez appliquer un clustering aux données de l'auto-encodeur. Celui qui aura le meilleur score gagne le TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vmauXUm1rKZ",
    "outputId": "b569da86-2be4-4573-b09f-73791d8201fa"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "train_data = data['data']\n",
    "train_labels = data['target']\n",
    "\n",
    "print(f\"Les {train_data.shape[0]} vecteurs sont de dimension {train_data.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_Zfr1cC1rKe"
   },
   "source": [
    "### Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRtyi0Eb1rKg"
   },
   "source": [
    "Il est souvent nécessaire de réduire la dimension des données que l'on manipule. Un encodeur (par exemple la première partie d'un auto-encodeur) est déjà un moyen de réduire les dimensions en codant des images dans un espace vectoriel beaucoup plus petit.\n",
    "Un auto-encodeur a le rôle très particulier d'encoder des images (ou autres données structurées) mais d'autres algorithmes s'appliquent à des données vectorielles classiques.\n",
    "\n",
    "La réduction de dimension permet à la fois à mieux visualiser les données (par exemple on peut réduire les vecteurs à 2 ou 3 dimensions pour afficher les données sur un graphique), et à la fois de pré-traiter les données. Pré-traiter les données en réduisant le nombre de dimensions est parfois indispensable, en effet, certains algorithmes - en particulier ceux basés sur la distances- supportent mal les très hautes dimensions.\n",
    "> Pour ceux qui ca intéresse, voir \"malédiction de la dimension\". [version simple (wiki)](<https://fr.wikipedia.org/wiki/Fl%C3%A9au_de_la_dimension>) / [version hardcore](<https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.64.2646&rep=rep1&type=pdf>).\n",
    "\n",
    "Dans la suite du TP, nous verrons la transformation t-SNE pour la visualisation, et l'ACP pour le pre-process.\n",
    "\n",
    "**Il est important de noter que les données que l'on utilise pour le TP n'ont que 13 dimensions. C'est minuscule et ça ne nécessite pas de réduire encore plus la dimension, l'ACP est là à titre pédagogique. Par contre, pour la dernière question du TP, l'ACP sera très utile.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyOvtPYj1rKj"
   },
   "source": [
    "#### Visualisation (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-IdZN7n1rKk"
   },
   "source": [
    "Le t-SNE (pour t-distributed stochastic neighbor embedding) est un algo de réduction de dimension spécialement conçu pour la visualisation de données. L'algorithme tend à maintenir les proximités entre les points depuis l'espace de grande dimension vers l'espace de petite dimension, selon une métrique donnée.\n",
    "\n",
    "> Si vous voulez creuser son fonctionnement (ça demande quelques connaissances en théorie de l'information) : \n",
    "> [version simple](<https://aiaspirant.com/introduction-to-t-distributed-stochastic-neighbor-embeddingt-sne/>) / [version complète](<https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf>).\n",
    "\n",
    "Notez que ca a été inventé par **Geoffrey Hinton**, un monument de l'IA, père de beaucoup d'autres méthodes. Vous allez probablement recroiser son nom souvent.\n",
    "\n",
    "Comme on va le voir, le t-SNE n'est pas déterministe (c'est-à-dire qu'il a une part d'aléatoire dans son exécution), et c'est une chose à ne pas oublier quand vous l'utiliserez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "2Y4ZC8J21rKl",
    "outputId": "997f84a7-5d96-48a0-c375-0c2e9c27d8a3"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "t0 = time()\n",
    "tsne_results = tsne.fit_transform(train_data)\n",
    "print(f\"Done in {time() - t0}s\")\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c=train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Transformation t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeMJLdZo1rKr"
   },
   "source": [
    "Le t-SNE n'est pas déterministe. Vous pouvez le vérifier en lançant plusieurs fois la réduction, vous n'aurez jamais le même résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbF6B1LFyjB1"
   },
   "source": [
    "#### Normalisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSp0K3Jxy5tE"
   },
   "source": [
    "Beaucoup d'algorithmes se basent sur les distances entre les données. Or la qualité des calculs des distances dépend fortement de la normalisation des axes. Par exemple, un axe qui a des valeurs entre 0 et 100 aura une influence beaucoup plus forte car ses valeurs sont beaucoup plus grandes que celles prises dans un axe ayant des valeurs entre 0 et 1, et on verrait une distorsion artificielle le long de l'axe non normalisé.\n",
    "\n",
    "Il y a principalement deux façons de normaliser, soit on ramène toutes les données entre un min et un max (MinMaxScaler), soit on normalise les données pour que chaque axe soit centré et réduit (StandardScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ev22rYrty3Kk",
    "outputId": "cf72416f-9a3b-4968-ccab-425b3202c38e"
   },
   "outputs": [],
   "source": [
    "for axe in range(train_data.shape[1]):\n",
    "  extract = train_data[:, [axe]]\n",
    "  print(f\"axe {axe} - min {min(extract)} - max {max(extract)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5_8PpI5zHQz"
   },
   "source": [
    "Les différentes composantes des vecteurs d'entrainement n'ont pas du tout les mêmes plages de valeurs. Pour vous entrainer à manipuler les transformateurs de normalisation, affichez en dessous trois transformations t-SNE, une avec les données brutes, une avec une normalisation MinMax et une avec une normalisation gaussienne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "id": "AsM_r5eDz7XO",
    "outputId": "1a08a86d-4b88-4a01-baef-87ffd2b8e8fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results_raw = tsne.fit_transform(train_data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_norm = scaler.fit_transform(train_data)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results_norm = tsne.fit_transform(train_data_norm)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_data_minmax = scaler.fit_transform(train_data)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results_minmax = tsne.fit_transform(train_data_minmax)\n",
    "\n",
    "\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "plt.scatter(tsne_results_raw[:,0], tsne_results_raw[:,1], c=train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"données non normées\")\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "plt.scatter(tsne_results_norm[:,0], tsne_results_norm[:,1], c=train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"données normées minmax\")\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "plt.scatter(tsne_results_minmax[:,0], tsne_results_minmax[:,1], c=train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"données normées standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnccCuIK1rKr"
   },
   "source": [
    "#### Réduction de dimension (ACP/PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fnqdtAq1rKs"
   },
   "source": [
    "L'Analyse en Composante Principale est une autre méthode de réduction de dimension. L'ACP repose sur un principe intuitif : on recherche les directions dans lesquelles les données s'étalent le plus.\n",
    "\n",
    "On vous encourage à regarder l'idée générale sur les liens ci-contre, ca se comprend très vite. \n",
    "[version simple (wiki)](<https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales>) / [version complète](<https://www.researchgate.net/publication/309165405_Principal_component_analysis_-_a_tutorial>).\n",
    "\n",
    "C'est la technique qui est de très loin la plus utilisée pour la réduction de dimension. Elle permet notamment de savoir combien d'information on perd lorsque l'on réduit la dimension. Elle a beaucoup d'autres avantages qui seront détaillés dans la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "EoF1O0aR1rKt",
    "outputId": "e081b267-a601-4c4b-d83e-a73e17e1c6de"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components, svd_solver='full', whiten=True)\n",
    "\n",
    "t0 = time()\n",
    "pca_results = pca.fit_transform(train_data_norm)\n",
    "print(f\"Done in {time() - t0}s\")\n",
    "\n",
    "components = range(1, n_components+1)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "plt.show()\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.plot(components, explained_variance)\n",
    "plt.xlabel(\"composantes\")\n",
    "plt.ylabel(\"proportion de variance expliquée\")\n",
    "plt.title(\"Variance expliquée par composante\")\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.plot(components, np.cumsum(explained_variance))\n",
    "print('\\nExplained variation per principal component: \\n {} \\n'.format(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"composantes\")\n",
    "plt.ylabel(\"proportion de variance expliquée cumulée\")\n",
    "plt.title(\"Variance expliquée cumulée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEmIsInt1rK0"
   },
   "source": [
    "Le graphe précédent montre la proportion de variance expliquée sur chacune des composantes principales. Intuitivement, il s'agit de la proportion d'information contenue dans chacune des composantes.\n",
    "Suivant vos besoins, vous pouvez donc garder les composantes principales qui vous sont utiles.\n",
    "En ce qui concerne les données que l'on manipule ici, on remarque que 6 dimensions sont nécessaires pour expliquer plus de 90% de la variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "jn2SyV261rK1",
    "outputId": "5860231f-5ee5-48b5-bb1c-bed39a2ba52c"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, svd_solver='full', whiten=True)\n",
    "\n",
    "t0 = time()\n",
    "pca_results = pca.fit_transform(train_data_norm)\n",
    "print(f\"Done in {time() - t0}s\")\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Les deux premières composantes de l'ACP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iarWZH9e1rK_"
   },
   "source": [
    "#### Pourquoi une ACP, pourquoi un t-SNE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3946NSGW1rLA"
   },
   "source": [
    "On l'a vu, le t-SNE a l'air d'être efficace pour réduire les dimensions, alors pourquoi on utilise une ACP pour le traitement réel des données ? \n",
    "\n",
    "C'est parce-que la transformation t-SNE souffre de quelques points noirs :\n",
    "\n",
    "- Le t-SNE, comme on l'a vu, n'est pas déterministe. La sortie change à chaque exécution, ce qui peut poser problème pour la manipulation des données après la transformation. L'ACP par contre fournit des résultats déterministes.\n",
    "\n",
    "- Le t-SNE se base sur les relations entre les points voisins mais ne permet pas toujours de visualiser les tendances globales. Vous aurez toujours des données bien étalées dans l'espace alors que les données d'entrée ne le sont pas forcément. C'est aussi ce qui rend le t-SNE parfois très utile pour visualiser les données.\n",
    "\n",
    "- Les composantes principales en sortie d'ACP portent en elles-mêmes une signification puisqu'elles se décomposent selon des composantes d'entrée. Si les données d'entrée ont un sens, alors les sorties auront un sens.\n",
    "> Par exemple, si on veut prédire le prix d'une maison, une ACP nous donne des informations du type : \"La composante principale qui explique le prix d'une maison est composée à 20% de sa localisation, à 30% de sa surface, ...\".\n",
    "\n",
    "- L'ACP permet d'obtenir les matrices de passage d'un espace à l'autre qui peuvent être utilisées pour projeter facilement de nouvelles données dans l'espace réduit.\n",
    "\n",
    "- L'ACP sert de base a beaucoup d'autres algorithmes, qui sont utiles pour des cas particuliers ou des cas extremes (peu de données, données mal réparties, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "oRvv03sh1rLC",
    "outputId": "43fb4cdd-090d-476e-c81f-12cb8661c969"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data_cancer = load_breast_cancer()\n",
    "train_data_cancer = data_cancer['data']\n",
    "train_labels_cancer = data_cancer['target']\n",
    "\n",
    "scaler_cancer = StandardScaler()\n",
    "train_data_cancer = scaler.fit_transform(train_data_cancer)\n",
    "\n",
    "pca_cancer = PCA(n_components=2, svd_solver='full', whiten=True)\n",
    "pca_results_cancer = pca.fit_transform(train_data_cancer)\n",
    "\n",
    "tsne_cancer = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results_cancer = tsne.fit_transform(train_data_cancer)\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.title(\"Transformation t-SNE\")\n",
    "plt.scatter(tsne_results_cancer[:,0], tsne_results_cancer[:,1], c=train_labels_cancer, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_results_cancer[:,0], pca_results_cancer[:,1], c=train_labels_cancer, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Transformation par ACP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H28aR5Z1rLB"
   },
   "source": [
    "En comparant les deux algorithmes, on remarque l'étalement provoqué par le t-SNE. C'est utile pour bien visualiser les données mais on perd l'information qui concerne les tendances globales des données. L'ACP donne des nuages de points plus denses ce qui les rend moins simple à visualiser, mais la tendance globale du nuage de point est bien visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRaTMJ4O1rLH"
   },
   "source": [
    "#### Quelques autres algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4nnsDcA1rLI"
   },
   "source": [
    "D'autres algorithmes permettent la réduction de dimension dans des cas particuliers :\n",
    "\n",
    "- La PLS (ou régression des moindres carrés partiels) est équivalente à l'ACP mais elle est supervisée. Ça permet de chercher les composantes principales qui permettent le mieux de distinguer les classes (qui sont donc connues).\n",
    "\n",
    "- L'algorithme FCA analyse les relations entre deux variable qualitatives. C'est donc une ACP pour des données d'entrée non numériques, et pour lesquelles on ne peut pas facilement définir de distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8Fd2YnK1rLX"
   },
   "source": [
    "### Classification non supervisée (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVMPafc31rLY"
   },
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq5kpEof1rLZ"
   },
   "source": [
    "Le K-means est un algorithme de clustering. Il fonctionne donc de manière non supervisé et se base sur une distance pour comparer les vecteurs.\n",
    "\n",
    "> Le K-means est un algo itératif. À l'initialisation, il prend k points répartis aléatoirement (que nous appelerons les *centroïdes*). Chaque itération comporte deux étapes :\n",
    "- On attribue chaque point des données à un des *centroïdes*. Dans sa forme basique, chaque donnée est attribuée au *centroïde* qui est le plus proche.\n",
    "- Pour chaque *centroïde*, on regroupe tous les points qui lui sont attribués, ceux-ci forment un cluster ; puis chaque *centroïde* est mis à jour comme étant le barycentre de son nouveau cluster.\n",
    "\n",
    ">  Ces étapes sont répétées jusqu'à l'arrêt par un critère de convergence.\n",
    "\n",
    "<img src=\"https://stanford.edu/~cpiech/cs221/img/kmeansViz.png\" width=\"440\" height=\"260\" align=\"center\"/>\n",
    "\n",
    "Pour avoir plus d'infos sur le K-means : [Anas Al-Masri : How does K-means work ?](<https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0>)\n",
    "\n",
    "\n",
    "Il a l'avantage d'être simple et rapide, mais nécessite de donner le nombre de clusters k, et c'est une information que l'on n'a pas toujours. De plus, le choix des valeurs initiales des *centroïdes* influent beaucoup sur le résultat.\n",
    "\n",
    "La solution classique pour pallier ce problème consiste à initialiser les *centroïdes* aléatoirement et lancer l'algorithme plusieurs fois, et avec différentes valeurs de k. Il existe aussi une variante kmeans++ qui intialise les *centroïdes* de manière à optimiser le résultat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "SqCKKjRz1rLZ",
    "outputId": "63fee34d-9820-4947-8942-51c4e28c45fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "prediction = kmeans.fit_predict(train_data_norm)\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Données annotées\")\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=prediction, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Predictions du K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INQLfswB1rLe"
   },
   "source": [
    "#### Clustering agglomératif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8q264iD1rLe"
   },
   "source": [
    "Le clustering agglomeratif consiste à constuire un arbre de similitude entre les données.\n",
    "Dans sa forme la plus simple, il s'agit de réunir itérativement les données les plus proches.\n",
    "\n",
    "> En deux mots. On initialise en considèrant chacun des points comme un cluster ayant un unique point. Ensuite à chaque étape on calcule les barycentres de chacun des clusters, et on regroupe les clusters dont les barycentres sont les plus proches.\n",
    "\n",
    "<img src=\"https://cedric.cnam.fr/vertigo/Cours/RCP216/_images/hierarchicalClustering.png\" width=\"440\" height=\"160\" align=\"center\"/>\n",
    "\n",
    "Pour en savoir plus : \n",
    "[version simple (wiki)](<https://fr.wikipedia.org/wiki/Regroupement_hi%C3%A9rarchique>) / [version complète](<lien>)\n",
    "\n",
    "Le grand avantage de cette technique c'est qu'elle ne nécessite pas de connaitre k en avance. On peut lancer l'algorithme et couper l'arbre au nombre de cluster qui nous convient ou qui donne le meilleurs score.\n",
    "\n",
    "Il existe différentes facon de regrouper les clusters, et différentes variantes. Par exemple, on peut aussi considérer tous nos points comme un seul grand cluster, puis le diviser itérativement de manière optimale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "9HvTDGV91rLf",
    "outputId": "562e1e06-03f9-4da4-9fe7-87431f18bc4a"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "AC = AgglomerativeClustering()\n",
    "AC.set_params(n_clusters=3)\n",
    "prediction = AC.fit_predict(train_data_norm)\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Données annotées\")\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=prediction, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Predictions de l'AC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BVrIig0K10S"
   },
   "source": [
    "La bbilothèque Scikit ne permet pas de visualiser le clustering hiérarchique. Ci-dessous on utilise Scipy pour afficher le dendogramme. On voit qu'il est assez facile de couper l'arbre à la bonne hauteur pour choisir le nombre de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "id": "wkEh5u5FJc_c",
    "outputId": "229c00a7-ff5b-4bd0-a1ff-b1ff81976afb"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "hc_complete = linkage(train_data_norm, \"complete\")\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('donnée')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    hc_complete,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJv-nx3m1rLk"
   },
   "source": [
    "#### Score NMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjBG-Mii1rLl"
   },
   "source": [
    "Trouver une bonne métrique pour mesurer la qualité d'un clustering non supervisé n'est pas si simple. En effet, la plupart des métriques classiques se contentent de comparer les *labels* prédits aux vrais labels. Mais dans notre cas, nous avons deux problèmes :\n",
    "\n",
    "  - En non-supervisé sur des données brutes, on n'a pas forcément ces *labels* vrais. Il faut donc se contenter de mesurer à quel point les classes sont denses (**inertie intra-cluster**), et à quel point les différentes classes sont éloignées (**inertie inter-cluster**). Il existe beaucoup de métriques de ce type, on peut citer le **coefficient de Silhouette** ou encore **l'indice de Calinski-Harabasz.**\n",
    "\n",
    "- Lorsque l'on a les labels vrais, ce n'est pas gagné non plus. En effet, si l'algorithme prédit [1, 1, 2, 2, 3, 3] alors que les vraies valeurs sont [3, 3, 1, 1, 2, 2], l'algorithme n'a pas fait d'erreur de clustering, mais une comparaison label à label donnerait un score nul. Il faut donc une métrique qui ne soit pas basée sur le nom des classes. On peut utiliser **l'Adjusted Rand Index (ARI)** ou encore **l'indice de Jacard**, mais pour le TP je vous propose le score **NMI (Normalized Mutual Information)**.\n",
    "\n",
    "\n",
    "\n",
    "> Pour plus d'informations sur les mesures de qualité du clustering, voir \n",
    "[Manimaran : Clustering Evaluation strategies](<https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "eW_5kiCH1rLm",
    "outputId": "243134ca-52d3-4a26-9f17-adfa0579079b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score as NMI\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "prediction = kmeans.fit_predict(train_data_norm)\n",
    "\n",
    "print(\"Score NMI :\", NMI(prediction, train_labels))\n",
    "\n",
    "#À vous de jouer ! Tracez le score en MNI en fonction du nombre de cluster (de 5 à 20) :\n",
    "\n",
    "cluster = []\n",
    "scores = []\n",
    "\n",
    "for k in range(1, 10):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "  prediction = kmeans.fit_predict(train_data_norm)\n",
    "  cluster.append(k)\n",
    "  score = NMI(prediction, train_labels)\n",
    "  scores.append(score)\n",
    "  print(f\"\\rk = {k}\", end='')\n",
    "\n",
    "plt.plot(cluster, scores)\n",
    "plt.xlabel(\"nombre de clusters\")\n",
    "plt.ylabel(\"score NMI\")\n",
    "plt.title(\"Qualité du clustering en fonction du nombre de clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Yy_-myLblix"
   },
   "source": [
    "On voit que le meilleur clustering se fait pour k=3. Il n'y a rien de très étonnant puisque nous avions trois classes dans les données. Lorsque vous manipulez des données sans connaitre k, cette méthode est un bon moyen de savoir quel est le nombre optimal de classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yAMdCr11rLr"
   },
   "source": [
    "### Pipeline sci-kit\n",
    "(cette sous-partie présente juste une astuce de programmation, peu connue, et très utile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRK2t55Z1rLr"
   },
   "source": [
    "La manière la plus propre d'utiliser sci-kit lorsque l'on a besoin d'un enchainement de plusieurs transformations, c'est d'utiliser les **pipelines**.\n",
    "\n",
    "Il s'agit d'un regroupement de plusieurs objets *transformers* réunis dans un seul objet ```Pipeline()```.\n",
    "\n",
    "```\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('nom_1', objet_transformateur_1()),\n",
    "    ('nom_2', objet_transformateur_2()),\n",
    "    etc...\n",
    "])\n",
    "```\n",
    "\n",
    "Pour manipuler votre Pipeline, vous pouvez utiliser les même méthodes que pour les transformateurs : ```.fit()```, ```.predict()```, ```.fit_predict()```, etc.\n",
    "\n",
    "> Vous pouvez aussi réécrire (en subclassing) les classes ```BaseEstimator```, ```TransformerMixin``` pour faire vos propres transformateurs et les inclure dans des pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soASMQ4ASX_q"
   },
   "source": [
    "Pour vous entrainer, essayez de  créer et entrainer une pipeline qui regroupe une ACP et un Kmeans sur les données ```train_data_viz```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmHudtR-1rLs",
    "outputId": "286126b9-525d-4d60-ef6e-0dad4592a6eb"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#ceci est un exemple de pipeline, mais en fait il est inutile de faire une ACP ici\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=10, svd_solver='full', whiten=True)),\n",
    "    ('kmeans', KMeans(n_clusters=3, random_state=0)),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(train_data_norm)\n",
    "prediction = pipeline.predict(train_data_norm)\n",
    "\n",
    "print(\"Score NMI :\", NMI(prediction, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tz9IVGiokPOi"
   },
   "source": [
    "# À vous de jouer en utilisant tout ça !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDN9cR9BTGoK"
   },
   "source": [
    "Pour finir le TP, essayez de faire une classification non-supervisée sur CIFAR-10. Celui qui a le score NMI le plus haut gagne le TP.\n",
    "\n",
    "> Indice :\n",
    "> Vous pouvez extraire des caractéristiques grace à un auto-encodeur, puis les classifier avec un algorithme de clustering de votre choix.\n",
    "\n",
    "N'oubliez pas que vous ne devrez pas utiliser les *labels*, sauf pour le calcul du score NMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCEYav9gcHD0"
   },
   "source": [
    "---\n",
    "PS : Si vous êtes motivés, vous pouvez coder un k-means *from scratch* (avec numpy), c'est pas hyper long et ca permet de bien comprendre l'algo."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
